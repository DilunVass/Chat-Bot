{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dilun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dilun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "#!pip install transformers datasets torch pandas numpy tqdm wandb gradio\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (995, 2)\n",
      "\n",
      "Sample data:\n",
      "Missing values:\n",
      "Training examples: 896\n",
      "Validation examples: 99\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/qa_dataset.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nSample data:\")\n",
    "df.head()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "df.isnull().sum()\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Basic cleaning\n",
    "        text = text.strip()\n",
    "        # Remove excessive whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Apply preprocessing\n",
    "df['Question'] = df['Question'].apply(preprocess_text)\n",
    "df['Answer'] = df['Answer'].apply(preprocess_text)\n",
    "\n",
    "# Create formatted text for GPT-2 fine-tuning\n",
    "# Format: \"Question: {question} Answer: {answer}\"\n",
    "df['formatted_text'] = 'Question: ' + df['Question'] + ' Answer: ' + df['Answer'] + '<|endoftext|>'\n",
    "\n",
    "# Split the data into training and validation sets (90-10 split)\n",
    "train_df = df.sample(frac=0.9, random_state=seed)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "print(f\"Training examples: {len(train_df)}\")\n",
    "print(f\"Validation examples: {len(val_df)}\")\n",
    "\n",
    "# Create text files for training and validation\n",
    "with open('train.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(train_df['formatted_text'].tolist()))\n",
    "    \n",
    "with open('val.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(val_df['formatted_text'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dilun\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load GPT-2 tokenizer and model\n",
    "model_name = \"gpt2\"  # We're using the base GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Adjust the tokenizer\n",
    "# GPT-2 doesn't have a padding token by default, so we'll set it\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create Dataset objects for training and validation\n",
    "def load_dataset(file_path, tokenizer, block_size=512):\n",
    "    return TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=file_path,\n",
    "        block_size=block_size\n",
    "    )\n",
    "\n",
    "train_dataset = load_dataset('train.txt', tokenizer)\n",
    "val_dataset = load_dataset('val.txt', tokenizer)\n",
    "\n",
    "# Create data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='54' max='54' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [54/54 11:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 11.53\n",
      "Model and tokenizer saved to ./gpt2-ctse-chatbot-final\n"
     ]
    }
   ],
   "source": [
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-ctse-chatbot\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    # Removed evaluation_strategy as it is not recognized\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if available\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\"  # Disable reporting to wandb or other services by default\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model_path = \"./gpt2-ctse-chatbot-final\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model and tokenizer saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CTSE Lecture Notes Chatbot ===\n",
      "\n",
      "Ask questions about CTSE topics. Type 'exit' to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./gpt2-ctse-chatbot-final\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "# Set the pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_answer(question, model, tokenizer, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given question using the fine-tuned GPT-2 model\n",
    "    \"\"\"\n",
    "    # Format the input text\n",
    "    input_text = f\"Question: {question.strip()} Answer:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer part\n",
    "    try:\n",
    "        answer = generated_text.split(\"Answer:\")[1].strip()\n",
    "        # Remove any additional questions that might be generated\n",
    "        if \"Question:\" in answer:\n",
    "            answer = answer.split(\"Question:\")[0].strip()\n",
    "    except IndexError:\n",
    "        answer = generated_text  # Return the whole text if we can't extract just the answer\n",
    "        \n",
    "    return answer\n",
    "\n",
    "# Create a simple chatbot interface\n",
    "def ctse_chatbot():\n",
    "    print(\"\\n=== CTSE Lecture Notes Chatbot ===\\n\")\n",
    "    print(\"Ask questions about CTSE topics. Type 'exit' to quit.\\n\")\n",
    "    \n",
    "    question = input(\"What is  meant by CNN?\")\n",
    "        \n",
    "    # Generate the answer\n",
    "    answer = generate_answer(question, model, tokenizer)\n",
    "    print(f\"\\nChatbot: {answer}\")\n",
    "\n",
    "# Run the chatbot\n",
    "if __name__ == \"__main__\":\n",
    "    ctse_chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d18edf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is meant by 'Creating a Service and exposing it' in context of DEMO?\n",
      "\n",
      "Answer: When you write a service, you create an instance of it. When the service is invoked, it creates a new instance and exposes it to the browser. You can use this approach to create a public API.\n",
      "\n",
      "Question\n",
      ". What can you do to make it easy for the developers to build a custom service?\n",
      "- Answer\n",
      " (If you are using an SDK you will need to use the 'Create a Simple Service' example below):\n",
      ": Create a simple service\n",
      ", , and expose it\n",
      "...\n",
      "( If you want to expose an API directly you need a framework like REST or JSON. Data is a container that\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./gpt2-ctse-chatbot-final\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "\n",
    "# Set the pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def generate_answer(question, model, tokenizer, max_length=150):\n",
    "    \"\"\"\n",
    "    Generate an answer for a given question using the fine-tuned GPT-2 model\n",
    "    \"\"\"\n",
    "    # Format the input text\n",
    "    input_text = f\"Question: {question.strip()} Answer:\"\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate the output\n",
    "    output = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer part\n",
    "    try:\n",
    "        answer = generated_text.split(\"Answer:\")[1].strip()\n",
    "        # Remove any additional questions that might be generated\n",
    "        if \"Question:\" in answer:\n",
    "            answer = answer.split(\"Question:\")[0].strip()\n",
    "    except IndexError:\n",
    "        answer = generated_text  # Return the whole text if we can't extract just the answer\n",
    "        \n",
    "    return answer\n",
    "\n",
    "# Hard-code the question and generate answer\n",
    "question = \"What is meant by 'Creating a Service and exposing it' in context of DEMO?\"\n",
    "print(f\"Question: {question}\")\n",
    "\n",
    "# Generate the answer\n",
    "answer = generate_answer(question, model, tokenizer)\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Run the code directly\n",
    "if __name__ == \"__main__\":\n",
    "    pass  # The code above will execute when the script runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ae717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
